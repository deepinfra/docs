---
title: Log Probabilities
description: Get per-token log probabilities from streaming LLM responses.
icon: chart-line
---

You can retrieve the log probability of each token generated by our streaming API. This is useful for tasks like uncertainty estimation, token-level filtering, or building custom sampling logic.

By default, the streaming API returns generated tokens one by one with their log probabilities attached.

## Example

```bash
curl -X POST \
    -d '{"input": "I have this dream", "stream": true}' \
    -H "Authorization: Bearer $DEEPINFRA_TOKEN" \
    -H 'Content-Type: application/json' \
    'https://api.deepinfra.com/v1/inference/meta-llama/Llama-2-7b-chat-hf'
```

Response (streamed):

```
data: {"token": {"id": 29892, "text": ",", "logprob": -2.65625, "special": false}, "generated_text": null, "details": null}
data: {"token": {"id": 988, "text": " where", "logprob": -0.39575195, "special": false}, "generated_text": null, "details": null}
data: {"token": {"id": 1432, "text": " every", "logprob": -3.15625, "special": false}, "generated_text": null, "details": null}
data: {"token": {"id": 931, "text": " time", "logprob": -0.1385498, "special": false}, "generated_text": null, "details": null}
```

The `logprob` field is the log probability of the generated token (base e). Lower (more negative) values indicate less likely tokens.

## Notes

- Log probabilities are returned via the [DeepInfra Native API](/apis/deepinfra-native) streaming endpoint (`/v1/inference/...`)
- Log probabilities are **not** currently returned for:
  - Non-streaming requests
  - The OpenAI-compatible API (`/v1/openai/...`)
