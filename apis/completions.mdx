---
title: Text Completions
description: Legacy OpenAI-compatible completions API for raw text generation.
icon: terminal
---

The completions API is the legacy text generation interface â€” you provide a raw prompt string and the model continues it. For most use cases, the [Chat Completions API](/chat/overview) is simpler and recommended instead.

The endpoint is:

```
POST https://api.deepinfra.com/v1/openai/completions
```

<Warning>
  This is an advanced API. You need to know your model's exact prompt format. Different models have different input formats. Check the model's API section on its page for the expected format.
</Warning>

## Example

The example below uses `meta-llama/Meta-Llama-3-8B-Instruct` with its Llama 3 prompt format:

<CodeGroup>

```python Python
from openai import OpenAI

openai = OpenAI(
    api_key="$DEEPINFRA_TOKEN",
    base_url="https://api.deepinfra.com/v1/openai",
)

stream = True  # or False

completion = openai.completions.create(
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    prompt="<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    stop=["<|eot_id|>"],
    stream=stream,
)

if stream:
    for event in completion:
        if event.choices[0].finish_reason:
            print(event.choices[0].finish_reason,
                  event.usage.prompt_tokens,
                  event.usage.completion_tokens)
        else:
            print(event.choices[0].text, end="", flush=True)
else:
    print(completion.choices[0].text)
    print(completion.usage.prompt_tokens, completion.usage.completion_tokens)
```

```javascript JavaScript
import OpenAI from "openai";

const openai = new OpenAI({
  baseURL: "https://api.deepinfra.com/v1/openai",
  apiKey: "$DEEPINFRA_TOKEN",
});

const stream = true; // or false

const completion = await openai.completions.create({
  model: "meta-llama/Meta-Llama-3-8B-Instruct",
  prompt: "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
  stream: stream,
  stop: ["<|eot_id|>"]
});

if (stream) {
  for await (const chunk of completion) {
    if (chunk.choices[0].finish_reason) {
      console.log(chunk.choices[0].finish_reason,
                  chunk.usage.prompt_tokens,
                  chunk.usage.completion_tokens);
    } else {
      process.stdout.write(chunk.choices[0].text);
    }
  }
} else {
  console.log(completion.choices[0].text);
  console.log(completion.usage.prompt_tokens, completion.usage.completion_tokens);
}
```

```bash cURL
curl "https://api.deepinfra.com/v1/openai/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $DEEPINFRA_TOKEN" \
  -d '{
     "model": "meta-llama/Meta-Llama-3-8B-Instruct",
     "prompt": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
     "stop": [
       "<|eot_id|>"
     ]
   }'
```

</CodeGroup>

## Supported parameters

| Parameter | Notes |
|-----------|-------|
| `model` | Model name or `MODEL_NAME:VERSION` |
| `prompt` | Raw prompt string in the model's expected format |
| `max_tokens` | |
| `stream` | |
| `temperature` | |
| `top_p` | |
| `stop` | |
| `n` | |
| `echo` | |
| `logprobs` | |

For every model, you can check its prompt format in the API section on its page.
